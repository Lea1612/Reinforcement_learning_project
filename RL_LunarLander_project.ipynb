{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RL_LunaLnder_v7_comments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HND9HYhOE89T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-cVn8qQKtJq"
      },
      "source": [
        "Based on the following links to create this notebook:\n",
        "\n",
        "https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H\n",
        "\n",
        "https://colab.research.google.com/drive/1tug_bpg8RwrFOI8C6Ed-zo0OgD3yfnWy#scrollTo=bhsj7BTPHepg\n",
        "\n",
        "https://colab.research.google.com/drive/1tug_bpg8RwrFOI8C6Ed-zo0OgD3yfnWy\n",
        "\n",
        "\n",
        "To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtbAjPPKiFw",
        "outputId": "009ecd0d-f486-4f70-f118-4ab5942480fe"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 0s (5,230 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 0s (8,729 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163330 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9be26c3790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_hxdz4ALVQ3"
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1GqN0iRLaZk"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "from math import exp\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from collections import deque, namedtuple\n",
        "from IPython import display as ipythondisplay\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import Adam\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILbriB_oLn5T"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnR-cqjbLpI4",
        "outputId": "bb237b37-8752-42fe-a807-aa1dd5a8e5ff"
      },
      "source": [
        "!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "yCHLYkMkLzcf",
        "outputId": "63b1cd1c-0bc0-4435-8cfb-50f69e2e8165"
      },
      "source": [
        "# Box2d Environment\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9b78ec5f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4v_3zCRepL",
        "outputId": "719da2d9-8259-40b5-b53b-6b9ff1791d47"
      },
      "source": [
        "state_size = env.observation_space\n",
        "print(\"state size is:\", state_size)\n",
        "a = env.action_space\n",
        "print(\"action size=\",a) \n",
        "state = env.reset()\n",
        "print(state)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state size is: Box(-inf, inf, (8,), float32)\n",
            "action size= Box(-1.0, 1.0, (2,), float32)\n",
            "[-4.8828126e-05  1.4078301e+00 -4.9733138e-03 -1.3733506e-01\n",
            "  6.3490952e-05  1.1265365e-03  0.0000000e+00  0.0000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfAdl7dSS-K"
      },
      "source": [
        "# Action Space\n",
        "            #is two floats [main engine, left-right engines].\n",
        "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
        "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
        "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9x29mQ0JIKu"
      },
      "source": [
        "# **Model DQN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4hW8JJQGaz"
      },
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    | DQN model with cusomize layers contains only fully connected layers with relu function\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hid_dim=(256, 256, 256)):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hid_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        current_dim = input_dim\n",
        "        for dim in hid_dim:\n",
        "            self.layers.append(nn.Linear(current_dim, dim))\n",
        "            current_dim = dim\n",
        "        self.output_layer = nn.Linear(current_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return self.output_layer(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMxZxDOhJToc"
      },
      "source": [
        "#**Model Dueling DQN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDZJhZBsZr95"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"\n",
        "    | Dueling DQN model with fully connected layers\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.feat_fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.value_fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.adv_fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feat_fc(x)\n",
        "        values = self.value_fc(x)\n",
        "        advantages = self.adv_fc(x)\n",
        "        return values + (advantages - advantages.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44z6n_bJf5d"
      },
      "source": [
        "# **Replay Memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkwU9oiuQKFr"
      },
      "source": [
        "Experience = namedtuple('Experience',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import random as rnd\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "\n",
        "Experience = namedtuple('Experience',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    \"\"\"\n",
        "    | replay memory to store the expririence for using in learning algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"\n",
        "        update memory queue\n",
        "        \"\"\"\n",
        "        self.position += 1\n",
        "        self.memory.append(Experience(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        | take sample in uniform way\n",
        "        \"\"\"\n",
        "        data = rnd.sample(self.memory, batch_size)\n",
        "        data = Experience(*zip(*data))\n",
        "        return (torch.cat(data.state).to(device),\n",
        "                torch.cat(data.action).to(device),\n",
        "                torch.cat(data.next_state).to(device),\n",
        "                torch.cat(data.reward).to(device))\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        | validate that we have enough samples to learn\n",
        "        \"\"\"\n",
        "        return self.position >= batch_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH-Z132jJkj4"
      },
      "source": [
        "# **Prioritized experience replay**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msEYNHcbiU5n"
      },
      "source": [
        "import random as rnd\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "Experience = namedtuple('Experience',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReplayMemoryPER(object):\n",
        "    # # beta_increment_per_sampling = 1.001\n",
        "    # # alpha_decay = 0.99\n",
        "    # beta = 0.5\n",
        "    # beta_increment_per_sampling = 0.001\n",
        "    # alpha = 0.5\n",
        "    # alpha_decay = 0.01\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        self.prioritize = deque(maxlen=capacity)\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"\n",
        "        update memory queue\n",
        "        :param args:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.position += 1\n",
        "        self.memory.append(Experience(*args))\n",
        "        self.prioritize.append(max(self.prioritize, default=1))\n",
        "\n",
        "    def get_probability(self):\n",
        "        \"\"\"\n",
        "        calculate probability to get choosen\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        scaled_priority = np.array(self.prioritize) ** 0.6\n",
        "        return scaled_priority / sum(scaled_priority)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        | take sample based on importance weight randomization\n",
        "        \"\"\"\n",
        "        sample_prob = self.get_probability()\n",
        "        indexes = rnd.choices(range(len(self.memory)), k=batch_size, weights=list(sample_prob))\n",
        "        data = np.array(self.memory)[indexes]\n",
        "        importance = self.get_importance(sample_prob[indexes])\n",
        "        data = Experience(*zip(*data))\n",
        "        return (torch.cat(data.state).to(device),\n",
        "                torch.cat(data.action).to(device),\n",
        "                torch.cat(data.next_state).to(device),\n",
        "                torch.cat(data.reward).to(device)), \\\n",
        "                importance, indexes\n",
        "\n",
        "    def get_importance(self, prob):\n",
        "        \"\"\"\n",
        "        getting sample importance\n",
        "        \"\"\"\n",
        "        importance = (1 / self.capacity * 1 / prob)**0.5\n",
        "        return importance\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        | validate that we have enough samples to learn\n",
        "        \"\"\"\n",
        "        return self.position >= batch_size\n",
        "\n",
        "    def set_priority(self, indices, errors, offset=1e-6):\n",
        "        \"\"\"\n",
        "        update priority for samples\n",
        "        \"\"\"\n",
        "        for i, e in zip(indices, errors):\n",
        "            self.prioritize[i] = abs(e) + offset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l16Dk_YJpnD"
      },
      "source": [
        "# **Agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5r_mLgkQRz6"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "TAU = 0.01\n",
        "LR = 0.0005\n",
        "TARGET_UPDATE = 4\n",
        "EPS_START = 0.99\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 0.01\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    \"\"\"\n",
        "    agent for choosing next step based on strategy\n",
        "    converting action to discrete value\n",
        "    update Q-values\n",
        "    save model\n",
        "    update target model\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, actions, model, strategy, full_model_name, loss=\"mse\"):\n",
        "        self.strategy = eval(\"self.\" + strategy)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = len(actions)\n",
        "        self._model = full_model_name\n",
        "        self._step = 0\n",
        "        self._actions = actions\n",
        "        self._memory = ReplayMemory(10000)\n",
        "        self.loss = F.mse_loss if loss == \"mse\" else F.smooth_l1_loss\n",
        "        self._policy_net = eval(model)(state_size, self.action_size).to(device)\n",
        "        self._target_net = eval(model)(state_size, self.action_size).to(device)\n",
        "        self._optimizer = optim.Adam(self._policy_net.parameters(), lr=LR)\n",
        "        self._mu = 0  # Mean\n",
        "        self._sigma = 0.05  # Std\n",
        "\n",
        "    def step(self, state, action, next_state, reward):\n",
        "        \"\"\"\n",
        "        add memory to queue\n",
        "        :param state: state of actor\n",
        "        :param action: action which was made\n",
        "        :param next_state: next step which the actor was moved\n",
        "        :param reward: reward according to state and action\n",
        "        \"\"\"\n",
        "        self._memory.push(torch.tensor([state]).to(device),\n",
        "                          torch.tensor([action]).long().to(device),\n",
        "                          torch.tensor([next_state]).to(device),\n",
        "                          torch.tensor([reward]).float().to(device))\n",
        "        self._step = (self._step + 1) % TARGET_UPDATE\n",
        "        # update each TARGET_UPDATE step\n",
        "        if self._step == 0:\n",
        "            if self._memory.can_provide_sample(BATCH_SIZE):\n",
        "                self.update_q_values()\n",
        "\n",
        "    def select_step(self, state, eps):\n",
        "        \"\"\"\n",
        "        Choosing next step based on strategy and epsilon\n",
        "        :param state: current state\n",
        "        :param eps: epsilon for choosing step\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.strategy(state, eps)\n",
        "\n",
        "    def add_noise(self):\n",
        "        \"\"\"\n",
        "        adding noise if configured\n",
        "        :return: gaussian noise\n",
        "        \"\"\"\n",
        "        return np.random.normal(self._mu, self._sigma, 2)\n",
        "\n",
        "    def soft_epsilon(self, state, eps):\n",
        "        \"\"\"\n",
        "        soft epsilon strategy choose by using model or choose by random\n",
        "        actions with weights which was given by model\n",
        "        :param state: current state\n",
        "        :param eps: epsilon threshold\n",
        "        \"\"\"\n",
        "        if rnd.uniform(0, 1) > eps:\n",
        "            with torch.no_grad():\n",
        "                action_index = self._policy_net(torch.tensor(state).to(device)).argmax().item()\n",
        "            self._policy_net.train()\n",
        "            return action_index\n",
        "        self._policy_net.eval()\n",
        "        with torch.no_grad():\n",
        "            weights = F.softmax(self._policy_net(torch.tensor(state).to(device)), dim=0)\n",
        "        self._policy_net.train()\n",
        "        return rnd.choices(np.arange(self.action_size), weights, k=1)[0]\n",
        "\n",
        "    def greedy_epsilon(self, state, eps):\n",
        "        \"\"\"\n",
        "        greedy policy choose action based on model or random action uniformly\n",
        "        :param state: current state\n",
        "        :param eps: epsilon threshold\n",
        "        \"\"\"\n",
        "        self._policy_net.eval()\n",
        "        if rnd.uniform(0, 1) > eps:\n",
        "            with torch.no_grad():\n",
        "                action_index = self._policy_net(torch.tensor(state).to(device)).argmax().item()\n",
        "            self._policy_net.train()\n",
        "            return action_index\n",
        "        else:\n",
        "            return rnd.choice(np.arange(self.action_size))\n",
        "\n",
        "    def action_index_to_value(self, act):\n",
        "        \"\"\"\n",
        "        convert result from model to discrete action\n",
        "        \"\"\"\n",
        "        return self._actions[act]\n",
        "\n",
        "    def update_q_values(self):\n",
        "        \"\"\"\n",
        "        update q-values of policy model\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not self._memory.can_provide_sample(BATCH_SIZE):\n",
        "            return\n",
        "        states, actions, next_states, rewards = self._memory.sample(BATCH_SIZE)\n",
        "        next_q, cur_q = self.get_q_values(states, next_states, actions)\n",
        "        target_q = next_q * GAMMA + rewards\n",
        "\n",
        "        loss = self.loss(cur_q, target_q.unsqueeze(-1))\n",
        "        self._optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "        self.soft_update()\n",
        "\n",
        "    def get_q_values(self, states, next_states, actions):\n",
        "        \"\"\"\n",
        "        calculate q-values\n",
        "        :param states: states from memory sample\n",
        "        :param next_states: next states from memory sample\n",
        "        :param actions: actions from memory sample\n",
        "        \"\"\"\n",
        "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "        non_final_states_locations = (final_state_locations == False)\n",
        "        non_final_states = next_states[non_final_states_locations]\n",
        "        values = torch.zeros(BATCH_SIZE).float().to(device)\n",
        "        with torch.no_grad():\n",
        "            if self._model in ['DDQN', 'DuelingDDQN']:\n",
        "                next_actions = self._policy_net(non_final_states).argmax(-1, keepdim=True)\n",
        "                values[non_final_states_locations] = self._target_net(non_final_states).gather(-1,\n",
        "                                                                                               next_actions).squeeze(-1)\n",
        "            else:\n",
        "                values[non_final_states_locations] = self._target_net(non_final_states).max(dim=1)[0]\n",
        "        return values, self._policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "    def soft_update(self):\n",
        "        \"\"\"\n",
        "        update the model softly by configuring TAU value\n",
        "        In case TAU equals 1 then it's hard update\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(self._target_net.parameters(), self._policy_net.parameters()):\n",
        "            target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "    def save_policy_net(self, name):\n",
        "        \"\"\"\n",
        "        save policy model\n",
        "        :param name: file name\n",
        "        \"\"\"\n",
        "        torch.save(self._policy_net.state_dict(), name+'.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO9_t-pEimsb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class AgentPER(object):\n",
        "    \"\"\"\n",
        "    Prioritized agent for choosing next step based on strategy\n",
        "    converting action to discrete value\n",
        "    update Q-values\n",
        "    save model\n",
        "    update target model\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, actions, model, strategy, full_model_name, loss=\"mse\"):\n",
        "        self.strategy = eval(\"self.\" + strategy)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = len(actions)\n",
        "        self._model = full_model_name\n",
        "        self._step = 0\n",
        "        self._actions = actions\n",
        "        self._memory = ReplayMemoryPER(10000)\n",
        "        self.loss = F.mse_loss if loss == \"mse\" else F.smooth_l1_loss\n",
        "        self._policy_net = eval(model)(state_size, self.action_size).to(device)\n",
        "        self._target_net = eval(model)(state_size, self.action_size).to(device)\n",
        "        self._optimizer = optim.Adam(self._policy_net.parameters(), lr=LR)\n",
        "        self._mu = 0  # Mean\n",
        "        self._sigma = 0.05  # Std\n",
        "\n",
        "    def step(self, state, action, next_state, reward):\n",
        "        \"\"\"\n",
        "        add memory to queue\n",
        "        :param state: state of actor\n",
        "        :param action: action which was made\n",
        "        :param next_state: next step which the actor was moved\n",
        "        :param reward: reward according to state and action\n",
        "        \"\"\"\n",
        "        self._memory.push(torch.tensor([state]).to(device),\n",
        "                          torch.tensor([action]).long().to(device),\n",
        "                          torch.tensor([next_state]).to(device),\n",
        "                          torch.tensor([reward]).float().to(device))\n",
        "        self._step = (self._step + 1) % TARGET_UPDATE\n",
        "        if self._step == 0:\n",
        "            if self._memory.can_provide_sample(BATCH_SIZE):\n",
        "                self.update_q_values()\n",
        "\n",
        "    def select_step(self, state, eps):\n",
        "        \"\"\"\n",
        "        Choosing next step based on strategy and epsilon\n",
        "        :param state: current state\n",
        "        :param eps: epsilon for choosing step\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self.strategy(state, eps)\n",
        "\n",
        "    def add_noise(self):\n",
        "        \"\"\"\n",
        "        adding noise if configured\n",
        "        :return: gaussian noise\n",
        "        \"\"\"\n",
        "        return np.random.normal(self._mu, self._sigma, 2)\n",
        "\n",
        "    def soft_epsilon(self, state, eps):\n",
        "        \"\"\"\n",
        "        soft epsilon strategy choose by using model or choose by random\n",
        "        actions with weights which was given by model\n",
        "        :param state: current state\n",
        "        :param eps: epsilon threshold\n",
        "        \"\"\"\n",
        "        self._policy_net.eval()\n",
        "        weights = F.softmax(self._policy_net(torch.tensor(state).to(device)), dim=0)\n",
        "        self._policy_net.train()\n",
        "        return rnd.choices(np.arange(self.action_size), weights, k=1)[0]\n",
        "\n",
        "    def greedy_epsilon(self, state, eps):\n",
        "        \"\"\"\n",
        "        greedy policy choose action based on model or random action uniformly\n",
        "        :param state: current state\n",
        "        :param eps: epsilon threshold\n",
        "        \"\"\"\n",
        "        self._policy_net.eval()\n",
        "        if rnd.uniform(0, 1) > eps:\n",
        "            with torch.no_grad():\n",
        "                action_index = self._policy_net(torch.tensor(state).to(device)).argmax().item()\n",
        "            self._policy_net.train()\n",
        "            return action_index\n",
        "        else:\n",
        "            return rnd.choice(np.arange(self.action_size))\n",
        "\n",
        "    def action_index_to_value(self, act):\n",
        "        \"\"\"\n",
        "        convert result from model to discrete action\n",
        "        \"\"\"\n",
        "        return self._actions[act]\n",
        "\n",
        "    def update_q_values(self):\n",
        "        \"\"\"\n",
        "        update q-values of policy model\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not self._memory.can_provide_sample(BATCH_SIZE):\n",
        "            return\n",
        "        (states, actions, next_states, rewards), weights, indexes = self._memory.sample(BATCH_SIZE)\n",
        "        next_q, cur_q = self.get_q_values(states, next_states, actions)\n",
        "        target_q = next_q * GAMMA + rewards\n",
        "\n",
        "        loss = self.loss(cur_q, target_q.unsqueeze(-1))\n",
        "        with torch.no_grad():\n",
        "            weight = np.multiply(weights, loss.data.cpu().numpy()).mean()\n",
        "        loss = weight * loss\n",
        "        self._optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "        self.soft_update()\n",
        "        delta = (abs(target_q.detach() - cur_q.squeeze(1).detach())).cpu().numpy()\n",
        "        self._memory.set_priority(indexes, delta)\n",
        "\n",
        "    def get_q_values(self, states, next_states, actions):\n",
        "        \"\"\"\n",
        "        calculate q-values\n",
        "        :param states: states from memory sample\n",
        "        :param next_states: next states from memory sample\n",
        "        :param actions: actions from memory sample\n",
        "        \"\"\"\n",
        "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "        non_final_states_locations = (final_state_locations == False)\n",
        "        non_final_states = next_states[non_final_states_locations]\n",
        "        values = torch.zeros(BATCH_SIZE).float().to(device)\n",
        "        with torch.no_grad():\n",
        "            if self._model in ['DDQN', 'DuelingDDQN']:\n",
        "                next_actions = self._policy_net(non_final_states).argmax(-1, keepdim=True)\n",
        "                values[non_final_states_locations] = self._target_net(non_final_states).gather(-1,\n",
        "                                                                                               next_actions).squeeze(-1)\n",
        "            else:\n",
        "                values[non_final_states_locations] = self._target_net(non_final_states).max(dim=1)[0]\n",
        "        return values, self._policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "    def soft_update(self):\n",
        "        \"\"\"\n",
        "        update the model softly by configuring TAU value\n",
        "        In case TAU equals 1 then it's hard update\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(self._target_net.parameters(), self._policy_net.parameters()):\n",
        "            target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "    def save_policy_net(self, name):\n",
        "        \"\"\"\n",
        "        save policy model\n",
        "        :param name: file name\n",
        "        \"\"\"\n",
        "        torch.save(self._policy_net.state_dict(), name + '.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDKRFdL-2GeB"
      },
      "source": [
        "# **Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt8tEQieZYzw"
      },
      "source": [
        "def plot_durations(data, name):\n",
        "    \"\"\"\n",
        "    plot episodes score and average of 100 episodes\n",
        "    :param data: episodes score\n",
        "    :param name: plot name to save\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    fig1 = plt.gcf()\n",
        "    data = torch.tensor(data, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.plot(data.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(data) >= 100:\n",
        "        means = data.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    plt.pause(0.0001)  # pause a bit so that plots are updated\n",
        "    fig1.savefig(name+\".png\")\n",
        "\n",
        "\n",
        "def plot_means(data):\n",
        "    \"\"\"\n",
        "    plot mean only graph\n",
        "    :param data: data to plot\n",
        "    \"\"\"\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    data = torch.tensor(data, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    if len(data) >= 100:\n",
        "        means = data.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePhx_86QJ5ZZ"
      },
      "source": [
        "# **Quantization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5djO2nROYuL"
      },
      "source": [
        "def get_engine(size, max_cont_val, min_cont_val):\r\n",
        "    \"\"\"\r\n",
        "    get possible values for engine\r\n",
        "    :param size: the size of action len\r\n",
        "    :param max_cont_val: max continuous value\r\n",
        "    :param min_cont_val: min continuous value\r\n",
        "    :return: possible discrete values\r\n",
        "    \"\"\"\r\n",
        "    min_cat_val = 0\r\n",
        "    cont_cat_size = abs(max_cont_val - min_cont_val) / (size - min_cat_val)\r\n",
        "    cont_val = [(min_cont_val + (discrete_val - min_cat_val + 1) * cont_cat_size)-cont_cat_size/2\r\n",
        "                               for discrete_val in range(size)]\r\n",
        "    return cont_val\r\n",
        "\r\n",
        "\r\n",
        "def get_actions():\r\n",
        "    \"\"\"\r\n",
        "    get possible actions by combination engines\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    return [[x, y] for x in [0]+get_engine(2, 1, 0)\r\n",
        "            for y in [0]+get_engine(2, 1, 0.5)+get_engine(2, -0.5, -1)]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1wLr6h0J8XI"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-azjZ6PWXTUf"
      },
      "source": [
        "def train(base_model, is_noise, strategy, full_model_name, get_next_step, loss, per):\n",
        "    \"\"\"\n",
        "    main train function with game iteration\n",
        "    \"\"\"\n",
        "    print(f\"\\n\\nCONFIGURATION:\\n\"\n",
        "          f\"\\t\\tFULL MODEL NAME: {full_model_name}\\n\"\n",
        "          f\"\\t\\tBASE MODEL:{base_model}\\n\"\n",
        "          f\"\\t\\tNOISE: {is_noise}\\n\"\n",
        "          f\"\\t\\tSTRATEGY: {strategy}\\n\"\n",
        "          f\"\\t\\tLOSS: {loss}\\n\"\n",
        "          f\"\\t\\tMEMORY INPUT: {get_next_step}\\n\"\n",
        "          f\"\\t\\tPRIORITIZED: {per}\\n\")\n",
        "    env = gym.make('LunarLanderContinuous-v2')\n",
        "    env.reset()\n",
        "    # discrete values for possible actions\n",
        "    actions = get_actions()\n",
        "    print(actions)\n",
        "    # select agent according to prioritized mode\n",
        "    if per:\n",
        "        agent = AgentPER(state_size=8 if get_next_step == \"single_snap\" else 16,\n",
        "                         actions=actions,\n",
        "                         model=base_model,\n",
        "                         strategy=strategy,\n",
        "                         full_model_name=full_model_name,\n",
        "                         loss=loss)\n",
        "    else:\n",
        "        agent = Agent(state_size=8 if get_next_step == \"single_snap\" else 16,\n",
        "                      actions=actions,\n",
        "                      model=base_model,\n",
        "                      strategy=strategy,\n",
        "                      full_model_name=full_model_name,\n",
        "                      loss=loss)\n",
        "    start = np.array([0, 0, 0, 0, 0, 0, 0, 0], dtype='float32')\n",
        "    scores_window = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for iteration in range(1000):\n",
        "        total_reward = 0\n",
        "        iteration += 1\n",
        "        done = False\n",
        "        if get_next_step == \"memory_snap\":\n",
        "            state = np.zeros(16, dtype=\"float32\")\n",
        "            state[:8] = env.reset()\n",
        "            state[8:] = start.copy()\n",
        "        else:\n",
        "            state = env.reset()\n",
        "        # update epsilon\n",
        "        new_eps = EPS_END + (EPS_START - EPS_END) * np.exp(-1 * iteration * EPS_DECAY)\n",
        "        while not done:\n",
        "            action = agent.select_step(state, new_eps)\n",
        "            action_value = agent.action_index_to_value(action)\n",
        "            next_state, reward, done, _ = env.step(action_value)\n",
        "            # add noise if configured\n",
        "            if is_noise:\n",
        "                next_state[:2] += agent.add_noise()\n",
        "            cur_state = state.copy()\n",
        "            # in case we use prev states\n",
        "            if get_next_step == \"memory_snap\":\n",
        "                state[14:] = state[12:14]    # prev prev prev prev\n",
        "                state[12:14] = state[10:12]  # prev prev prev\n",
        "                state[10:12] = state[8:10]   # prev prev\n",
        "                state[8:10] = state[:2]      # prev\n",
        "                state[:8] = next_state\n",
        "            # in case the state contains only next state\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "            agent.step(cur_state, action, state, reward)\n",
        "            total_reward += reward\n",
        "        scores_window.append(total_reward)\n",
        "        scores.append(total_reward)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f} EPS:{}'.format(iteration, np.mean(scores_window), new_eps), end=\"\")\n",
        "        if iteration % 100 == 0 and iteration > 0 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(iteration, np.mean(scores_window)))\n",
        "        if np.mean(scores_window) >= 200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(iteration,\n",
        "                                                                                         np.mean(scores_window)))\n",
        "            name = f\"{full_model_name}_\" \\\n",
        "                   f\"{get_next_step}_per={per}\" \\\n",
        "                   f\"loss_{loss}_{strategy}_\" \\\n",
        "                   f\"is_noise={is_noise}_ep_{iteration}_\" \\\n",
        "                   f\"{datetime.now().hour}_{datetime.now().minute}\"\n",
        "            plot_durations(scores, name)\n",
        "            agent.save_policy_net(name)\n",
        "            break\n",
        "        agent.soft_update()\n",
        "    plot_means(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFeeYYJTh87M"
      },
      "source": [
        "#Dueling, noise, PER, huber\r\n",
        "\r\n",
        "train(base_model=\"DuelingDQN\",  # can be DQN, DuelingDQN\r\n",
        "      is_noise=False,  # True, False\r\n",
        "      strategy=\"greedy_epsilon\",  # greedy_epsilon or soft_epsilon\r\n",
        "      full_model_name=\"DuelingDDQN\",  # DQN, DDQN, DuelingDDQN\r\n",
        "      get_next_step=\"single_snap\",  # memory_snap or single_snap\r\n",
        "      loss=\"mse\",  # huber or mse\r\n",
        "      per=False)  # Prioritized True or False\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}